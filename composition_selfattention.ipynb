{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 80])\n",
      "torch.Size([80, 40])\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "import ast\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data_utils\n",
    "import models\n",
    "import importlib\n",
    "import transformer_models\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(models)\n",
    "importlib.reload(transformer_models)\n",
    "from data_utils import *\n",
    "from models import *\n",
    "from transformer_models import * \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core.periodic_table import Element\n",
    "MAX_ATOM = 80\n",
    "# List of atomic numbers\n",
    "atomic_numbers = list(range(1, MAX_ATOM))\n",
    "\n",
    "# Dictionary to store atomic radii\n",
    "atomic_radii = []\n",
    "\n",
    "# Iterate over the atomic numbers and retrieve atomic radii\n",
    "for atomic_number in atomic_numbers:\n",
    "    element = Element.from_Z(atomic_number)\n",
    "    atomic_radius = element.atomic_radius\n",
    "    atomic_radii.append(atomic_radius)\n",
    "\n",
    "#replace None entries with 0 \n",
    "atomic_radii = [elem if elem != None else 0 for elem in atomic_radii ]\n",
    "\n",
    "atomic_radii = np.array(atomic_radii)\n",
    "\n",
    "from pymatgen.core.periodic_table import Element\n",
    "\n",
    "# Get a list of all element symbols\n",
    "element_symbols = [elem.symbol for elem in Element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = torch.load('data/train.pt')\n",
    "train_df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['atomic_numbers'] = train_df['atomic_numbers'].apply(ast.literal_eval)\n",
    "atomic_num_list = [np.array(sublist) for sublist in list(train_df['atomic_numbers'])]\n",
    "indices_to_exclude = [i for i, val in enumerate(atomic_num_list) if np.any(val > MAX_ATOM-1)]\n",
    "filtered_df = train_df.drop(indices_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_num_list = [np.array(sublist) for sublist in list(filtered_df['atomic_numbers'])]\n",
    "spacegroup_list = torch.tensor(list(filtered_df['spacegroup.number']))\n",
    "\n",
    "all_atom_types = [np.concatenate([vec, np.zeros(25 - len(vec))]) for vec in atomic_num_list]\n",
    "all_atom_types = torch.tensor(np.stack(all_atom_types)).long()\n",
    "training_data_onehot = torch.nn.functional.one_hot(all_atom_types, num_classes=MAX_ATOM).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (Tensor): The input tensor of shape (n, 25, 40)\n",
    "            targets (Tensor): The target tensor of shape (n, 1)\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ElemFormer().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "dataObject = xrdData(\"data/\", device)\n",
    "\n",
    "dataObject.make_datasets(1, composition_embedding=\"compositionseq\")\n",
    "# Create DataLoaders for train and validation sets\n",
    "train_loader = DataLoader(dataObject.torch_datasets['train'], batch_size=256, shuffle=True)\n",
    "valid_loader = DataLoader(dataObject.torch_datasets['val'], batch_size=256, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"data/\"\n",
    "# dataObject = xrdData(\"data/\", device)\n",
    "\n",
    "# # Create DataLoaders for train and validation sets\n",
    "# train_loader = DataLoader(CustomDataset(training_data_onehot[:-1000], spacegroup_list[:-1000]), batch_size=256, shuffle=True, pin_memory=True)\n",
    "# val_loader = DataLoader(CustomDataset(training_data_onehot[1000:], spacegroup_list[1000:]), batch_size=256, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the model instance and move it to the selected device\u001b[39;00m\n\u001b[1;32m      2\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m230\u001b[39m  \u001b[38;5;66;03m# Output dimension\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m ConvModel \u001b[38;5;241m=\u001b[39m \u001b[43mXRD_C_SymNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomposition_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m token_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;66;03m#dimension of the tokens \u001b[39;00m\n\u001b[1;32m      5\u001b[0m TransModel \u001b[38;5;241m=\u001b[39m TransformerModel(ntoken \u001b[38;5;241m=\u001b[39m output_dim, d_model \u001b[38;5;241m=\u001b[39m token_size, nhead \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, d_hid\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, nlayers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Miscellaneous/452_final_project/models.py:161\u001b[0m, in \u001b[0;36mXRD_C_SymNet.__init__\u001b[0;34m(self, in_channels, output_dim, composition_model, xrd_model)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28msuper\u001b[39m(XRD_C_SymNet, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xrd_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxrd_module \u001b[38;5;241m=\u001b[39m \u001b[43mXRD_convnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxrd_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Miscellaneous/452_final_project/models.py:28\u001b[0m, in \u001b[0;36mXRD_convnet.__init__\u001b[0;34m(self, in_channels, output_dim)\u001b[0m\n\u001b[1;32m     24\u001b[0m  \u001b[38;5;66;03m# Calculate flattened_size dynamically\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattened_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_flattened_size(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, in_channels, \u001b[38;5;241m8500\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMLP \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflattened_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2300\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     29\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     30\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     31\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m2300\u001b[39m, \u001b[38;5;241m1150\u001b[39m),\n\u001b[1;32m     32\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m     33\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     34\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1150\u001b[39m, output_dim)\n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/cdvae/lib/python3.8/site-packages/torch/nn/modules/linear.py:84\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cdvae/lib/python3.8/site-packages/torch/nn/modules/linear.py:87\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/.conda/envs/cdvae/lib/python3.8/site-packages/torch/nn/init.py:379\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    377\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the model instance and move it to the selected device\n",
    "output_dim = 230  # Output dimension\n",
    "ConvModel = XRD_C_SymNet(in_channels=1, output_dim=output_dim, composition_model= None).to(device)\n",
    "token_size = 50 #dimension of the tokens \n",
    "TransModel = TransformerModel(ntoken = output_dim, d_model = token_size, nhead = 10, d_hid=50, nlayers=1, dropout = 0.5).to(device)\n",
    "model = TransModel\n",
    "# Define optimizer and loss function\n",
    "weight_decay = 0  # Example value, adjust based on your needs\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "weight_decay = 0  # Example value, adjust based on your needs\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate experimental data simulator \n",
    "simulator = ExperimentalSimulation(device, crop_start=2000, crop_stop = 2000, noise_range = 0.4, drop_width = 500, drop_freq = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31369900"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(ConvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148156370"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(TransModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21173507423271776"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(ConvModel) / count_parameters(TransModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:23<00:00,  5.09it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 19.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 3.744630541404088, Training Accuracy: 17.31045751633987%, Validation Loss: 3.844312906265259, Validation Accuracy: 18.35836354220611%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 12/120 [00:02<00:23,  4.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 22\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m total_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epochs = 200\n",
    "metrics = [\"accuracy\", \"loss\"]\n",
    "\n",
    "log = {\n",
    "    f\"{type}\": {f\"{metric}\" : np.zeros(max_epochs) for metric in metrics} for type in ['train', 'val']     \n",
    "}\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for xrd, composition, targets in tqdm(train_loader): \n",
    "        #xrd = simulator.sim(xrd)\n",
    "        #xrd = F.normalize(xrd, p=2, dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xrd, composition)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += targets.size(0)\n",
    "        correct_train += (predicted == targets).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_valid_loss = 0\n",
    "    correct_valid = 0\n",
    "    total_valid = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for xrd, composition, targets in tqdm(valid_loader):\n",
    "            #xrd = simulator.sim(xrd)\n",
    "            #xrd = F.normalize(xrd, p=2, dim=1)\n",
    "            outputs = model(xrd, composition)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_valid_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1) \n",
    "            total_valid += targets.size(0)\n",
    "            correct_valid += (predicted == targets).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * correct_valid / total_valid\n",
    "\n",
    "    total_train_loss = total_train_loss / len(train_loader)\n",
    "    validation_loss = total_valid_loss / len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {total_train_loss}, Training Accuracy: {train_accuracy}%, Validation Loss: {validation_loss}, Validation Accuracy: {valid_accuracy}%\")\n",
    "\n",
    "    log['train']['accuracy'][epoch] = (train_accuracy)\n",
    "    log['train']['loss'][epoch] =(total_train_loss)\n",
    "\n",
    "    log['val']['accuracy'][epoch] =(valid_accuracy)\n",
    "    log['val']['loss'][epoch] =(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence  = torch.ones((1, 2, 5)).to(device)\n",
    "test_sequence[:, 1, :2] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [2., 2., 1., 1., 1.]]], device='cuda:0')"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = transformer_encoder(test_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
