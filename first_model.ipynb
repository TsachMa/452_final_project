{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pvs = torch.load(\"train_pvs.pt\")\n",
    "val_pvs = torch.load(\"val_pvs.pt\")\n",
    "training_sgs = torch.load(\"train_sgs.pt\")\n",
    "val_sgs = torch.load(\"val_sgs.pt\")\n",
    "train_composition = torch.load(\"train_composition.pt\")\n",
    "val_composition = torch.load(\"val_composition.pt\")\n",
    "\n",
    "train_composition2D = torch.load(\"train_composition2D.pt\")\n",
    "val_composition2D = torch.load(\"val_composition2D.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pvs = training_pvs.to(device)\n",
    "training_sgs = training_sgs.to(device)\n",
    "val_pvs = val_pvs.to(device)\n",
    "val_sgs = val_sgs.to(device)\n",
    "\n",
    "train_composition = train_composition.to(device).float()\n",
    "val_composition = val_composition.to(device).float()\n",
    "\n",
    "train_composition2D = train_composition2D.to(device).float()\n",
    "val_composition2D = val_composition2D.to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_composition.mean()\n",
    "std  = train_composition.std()\n",
    "\n",
    "train_composition = (train_composition - mean ) / std\n",
    "val_composition = (val_composition - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class XRD_and_composition(Dataset):\n",
    "    def __init__(self, xrd, composition, targets):\n",
    "        assert xrd.size(0) == composition.size(0) == targets.size(0), \"The number of elements in both tensor sets should be the same\"\n",
    "        self.xrd = xrd\n",
    "        self.composition = composition\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.xrd.size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.xrd[index], self.composition[index], self.targets[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_composition.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shrink the training data for sandboxing \n",
    "fraction_of_total_data = 1\n",
    "amt_of_data = int(fraction_of_total_data * train_composition.shape[0])\n",
    "\n",
    "train_dataset = XRD_and_composition(training_pvs[:amt_of_data], train_composition[:amt_of_data], training_sgs[:amt_of_data])\n",
    "val_dataset = TensorDataset(val_pvs, val_composition, val_sgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for train and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRD_convnet(nn.Module):\n",
    "    def __init__(self, in_channels, output_dim):\n",
    "        super(XRD_convnet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 80, kernel_size = 100, stride=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.Conv1d(80, 80, 50, stride=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.Conv1d(80, 80, 25, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(80),\n",
    "        )\n",
    "\n",
    "         # Calculate flattened_size dynamically\n",
    "        self.flattened_size = self._get_flattened_size(input_shape=(1, in_channels, 8500))\n",
    "\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 2300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2300, 1150),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1150, output_dim)\n",
    "        )\n",
    "\n",
    "       \n",
    "\n",
    "    def _get_flattened_size(self, input_shape):\n",
    "        dummy_input = torch.zeros(input_shape)\n",
    "        with torch.no_grad():\n",
    "            dummy_output = self.conv_layers(dummy_input)\n",
    "        return int(np.prod(dummy_output.shape))\n",
    "    \n",
    "    def forward(self, x,): \n",
    "\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.MLP(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class composition_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(composition_MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.composition_net = nn.Sequential(\n",
    "            nn.Linear(100, 230),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(230),  \n",
    "            nn.Linear(230, 230),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(230),  \n",
    "            nn.Linear(230, 230),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(230),  \n",
    "            nn.Linear(230, 230),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, c): \n",
    "        c = c.squeeze(1)\n",
    "        c = self.composition_net(c)\n",
    "        return c\n",
    "\n",
    "class composition_CNN(nn.Module):\n",
    "    def __init__(self, in_channels, output_dim):\n",
    "        super(composition_CNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.composition_convnet = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, 5, 1, 0),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20),\n",
    "            nn.Conv2d(20, 10, 3, 1, 0),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.Conv2d(10, 5, 3, 1, 0),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(5),\n",
    "        )\n",
    "\n",
    "        self.flattened_size2 = self._get_flattened_comp_size(input_shape=(1, 1, 21, 100))\n",
    "\n",
    "        self.post_composition_convnet = nn.Sequential(\n",
    "                nn.Linear(self.flattened_size2, 230),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(230, 230),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(230, 230),\n",
    "        )\n",
    "\n",
    "    def _get_flattened_comp_size(self, input_shape):\n",
    "        dummy_input = torch.zeros(input_shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_output = self.composition_convnet(dummy_input)\n",
    "        \n",
    "        return int(np.prod(dummy_output.shape))\n",
    "    \n",
    "    def forward(self, c): \n",
    "        c = self.composition_convnet(c)\n",
    "        c = self.flatten(c)\n",
    "        c = self.post_composition_convnet(c)\n",
    "        return c\n",
    "\n",
    "\n",
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, output_dim, xrd_type = \"cnn\", composition_type = \"mlp\"):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "\n",
    "        if xrd_type == \"cnn\":\n",
    "            self.xrd_module = XRD_convnet(in_channels, output_dim)\n",
    "        else:\n",
    "            self.xrd_module = None\n",
    "        \n",
    "        if composition_type == \"mlp\":\n",
    "            self.composition_module = composition_MLP()\n",
    "        elif composition_type == \"cnn\":\n",
    "            self.composition_module = composition_CNN()\n",
    "        else:\n",
    "            self.composition_module = None\n",
    "    \n",
    "        if (xrd_type is not None) and (composition_type is not None):\n",
    "            self.merge_net = nn.Sequential(\n",
    "                nn.Linear(460, 230),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(230, 230)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, c, nettype = None):\n",
    "        if self.xrd_module: \n",
    "            x = self.xrd_module(x)\n",
    "    \n",
    "        if self.composition_module:\n",
    "            c = self.composition_module(c)\n",
    "\n",
    "        if self.merge_net:\n",
    "            m = torch.cat((x, c), dim = 1)\n",
    "            m = self.merge_net(m)\n",
    "            return m\n",
    "        \n",
    "        elif self.xrd_module:\n",
    "            return x\n",
    "        elif self.composition_module:\n",
    "            return c    \n",
    "    \n",
    "# Create the model instance and move it to the selected device\n",
    "output_dim = 230  # Output dimension\n",
    "model = SimpleConvNet(in_channels=1, output_dim=output_dim).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "weight_decay = 0  # Example value, adjust based on your needs\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 3.352157087190777, Training Accuracy: 25.967894239848913%, Validation Loss: 2.8495099544525146, Validation Accuracy: 32.43422507185496%\n",
      "Epoch 2, Training Loss: 2.3561569122557944, Training Accuracy: 42.279064600344384%, Validation Loss: 2.0758049223158093, Validation Accuracy: 47.99911563121822%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     20\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/cdvae/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cdvae/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cdvae/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    107\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/cdvae/lib/python3.8/site-packages/torch/optim/_functional.py:92\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     94\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m     96\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_accuracy_log = []\n",
    "train_loss_log = []\n",
    "val_accuracy_log = []\n",
    "val_loss_log = []\n",
    "mode = \"xrd\"\n",
    "for epoch in range(100):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for xrd, composition, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xrd, composition, mode)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += targets.size(0)\n",
    "        correct_train += (predicted == targets).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_valid_loss = 0\n",
    "    correct_valid = 0\n",
    "    total_valid = 0\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed for validation\n",
    "        for xrd, composition, targets in valid_loader:\n",
    "            outputs = model(xrd, composition, mode)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_valid_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1) \n",
    "            total_valid += targets.size(0)\n",
    "            correct_valid += (predicted == targets).sum().item()\n",
    "\n",
    "    valid_accuracy = 100 * correct_valid / total_valid\n",
    "\n",
    "    total_train_loss = total_train_loss / len(train_loader)\n",
    "    validation_loss = total_valid_loss / len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {total_train_loss}, Training Accuracy: {train_accuracy}%, Validation Loss: {validation_loss}, Validation Accuracy: {valid_accuracy}%\")\n",
    "\n",
    "    train_accuracy_log.append(train_accuracy)\n",
    "    train_loss_log.append(total_train_loss)\n",
    "\n",
    "    val_accuracy_log.append(valid_accuracy)\n",
    "    val_loss_log.append(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction_val_accuracy_informal = {0.2: (95.22, 47.33),\n",
    "                                        0.4: (94.785, 57.373),\n",
    "                                        0.7: (94.165, 65.001),\n",
    "                                        1: (94.07876, 68.295)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
