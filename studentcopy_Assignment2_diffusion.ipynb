{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eOxhRX-wEXc"
   },
   "source": [
    "# **Deep Learning Theory and Application** Assignment 2\n",
    "# Coding Exercise on Diffusion Model\n",
    "\n",
    "### Name: Tsach Mackey\n",
    "### NetID: tcm57\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBuhvlsQJ51Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This is a hands-on introduction to the simplest diffusion model.\n",
    "\n",
    "## Scenario\n",
    "Image generation.\n",
    "\n",
    "## Method\n",
    "We will be implementing [Denoising Diffusion Probablistic Model (DDPM)](https://arxiv.org/abs/2006.11239).\n",
    "\n",
    "Meanwhile, it is good to read several other papers, such as [Stable Diffusion](https://arxiv.org/abs/2112.10752).\n",
    "\n",
    "\n",
    "## Goal\n",
    "Please fill in the TODO items and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpksDIXWwnsG",
    "outputId": "7d7e1e37-4a56-40ce-faaf-15ad4a39286e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn, einsum\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfs\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isfunction\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "# !pip install einops\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as tfs\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import urllib\n",
    "\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMB5MS5Hb7j8"
   },
   "source": [
    "## 1. Defining the U-Net model for diffusion.\n",
    "\n",
    "You don't have to worry about the details of this architecture, except that it contains a time embedding module which is essential to the diffusion model.\n",
    "\n",
    "**You shall focus on the diffusion process.**\n",
    "\n",
    "The illustration below is from the stable diffusion paper. It nicely depicts the architecture.\n",
    "\n",
    "However, please note that DDPM that we are implementing does not have those \"conditioning\" as in stable diffusion.\n",
    "\n",
    "Side note: what are conditioning? These are inputs to the model during inference stage that forces the model to generate the output conditioned on them. For example, in Dalle (native image generation support in GPT-4 chatbot), you can send it a text prompt and ask it to generate an image based on the text description you provide.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f6/Stable_Diffusion_architecture.png\" width=\"600\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7My14At7iWA8"
   },
   "outputs": [],
   "source": [
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "\n",
    "def Upsample(dim, dim_out=None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "        nn.Conv2d(dim, default(dim_out, dim), 3, padding=1),\n",
    "    )\n",
    "\n",
    "\n",
    "def Downsample(dim, dim_out=None):\n",
    "    # No More Strided Convolutions or Pooling\n",
    "    return nn.Sequential(\n",
    "        Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1),\n",
    "    )\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class WeightStandardizedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/1903.10520\n",
    "    weight standardization purportedly works synergistically with group normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n",
    "\n",
    "        weight = self.weight\n",
    "        mean = reduce(weight, \"o ... -> o 1 1 1\", \"mean\")\n",
    "        var = reduce(weight, \"o ... -> o 1 1 1\", partial(torch.var, unbiased=False))\n",
    "        normalized_weight = (weight - mean) / (var + eps).rsqrt()\n",
    "\n",
    "        return F.conv2d(\n",
    "            x,\n",
    "            normalized_weight,\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups,\n",
    "        )\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)\n",
    "\n",
    "# Define a neural network for the reverse diffusion process\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        self_condition=False,\n",
    "        resnet_block_groups=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0) # changed to 1 and 0 from 7,3\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # layers\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Downsample(dim_in, dim_out)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.out_dim = default(out_dim, channels)\n",
    "\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond=None):\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim=1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l09TbYyxcEOn"
   },
   "source": [
    "## 2. Implementing the diffusion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acTGW772zMYF"
   },
   "source": [
    "### Forward process\n",
    "\n",
    "Recall that in forward diffusion, given a data point sampled from a data distribution $x_0 \\sim q(x)$, we want to get a noisy image at step $T$.\n",
    "\n",
    "In particular, the forward diffusion is controlled by a variance schedule $\\{ \\beta_t \\in (0, 1) \\}_{t=1}^{T}$.\n",
    "\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}  \\textrm{ } x_{t-1}, \\beta_t \\textrm{ } \\mathbf{I})$$\n",
    "$$q(x_{1:T} | x_0) = \\prod_{t=1}^T q(x_t | x_{t-1})$$\n",
    "\n",
    "The sampled image $x_0$ gradually becomes noisier as the step $t$ becomes larger.\n",
    "\n",
    "Eventually when $T \\rightarrow \\infty$, $x_T$ becomes to Gaussian noise. This is a Markov process and we can apply the formulation $T$ times to get to a noisy image at timestep $T$. If we do this in a Markovian manner, the process is slow and expensive.\n",
    "\n",
    "Luckily, we can sample $x_t$ at any step $t$ **directly** using the reparameterization trick.\n",
    "\n",
    "Let $\\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{i=1}^t \\alpha_i$\n",
    "\n",
    "Then\n",
    "\n",
    "$$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I})$$\n",
    "\n",
    "\n",
    "### Reverse process (sampling)\n",
    "\n",
    "Algorithm 2 in the paper.\n",
    "\n",
    "$x_T \\sim \\mathcal{N}(0, \\mathbf{I})$\n",
    "\n",
    "**for** $t = T, ..., 1$ **do**\n",
    "\n",
    "$\\hspace{16pt} z \\sim \\mathcal{N}(0, \\mathbf{I})$ if $t > 1$, else $z = 0$\n",
    "\n",
    "$\\hspace{16pt} x_{t-1} = \\mu_{\\theta}(x_t, t) + \\sigma_t z = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_{\\theta} (x_t, t) \\right) + \\sigma_t z$\n",
    "\n",
    "**end for**\n",
    "\n",
    "**return** x_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkkkHyWozMYF"
   },
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "\n",
    "class Diffusion(object):\n",
    "    '''\n",
    "    All-in-one place for diffusion.\n",
    "    '''\n",
    "    def __init__(self, timesteps: int = 300):\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = self.linear_beta_schedule()\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "\n",
    "    def linear_beta_schedule(self):\n",
    "        beta_start = 0.0001\n",
    "        beta_end = 0.02\n",
    "        return torch.linspace(beta_start, beta_end, self.timesteps)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        '''\n",
    "        Forward process\n",
    "        '''\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "\n",
    "        # TODO: fill in this!\n",
    "        # Hint: \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\mathbf{I}\n",
    "        q = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x, t, t_index):\n",
    "        '''\n",
    "        Reverse process\n",
    "        '''\n",
    "        betas_t = extract(self.betas, t, x.shape)\n",
    "\n",
    "        sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "            sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "        )\n",
    "\n",
    "        sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
    "        sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "        # Use our model (noise predictor) to predict the mean\n",
    "        # TODO: Fill in this!\n",
    "        # Hint: this is \\mu_{\\theta}(x_t, t).\n",
    "        # Hint: Equation 11 in the paper.\n",
    "        model_mean = sqrt_recip_alphas_t * (x - betas_t * (1 / sqrt_one_minus_alphas_cumprod_t) * model(x, t))\n",
    "\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "            posterior_variance = self.betas * (1. - alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "            posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            # TODO: Fill in this!\n",
    "            # Hint: Algorithm 2 line 4\n",
    "            p = model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "            return p\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape):\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "        b = shape[0]\n",
    "        # start from pure noise (for each example in the batch)\n",
    "        img = torch.randn(shape, device=device)\n",
    "        imgs = []\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.timesteps)), desc='sampling loop time step', total=self.timesteps):\n",
    "            img = self.p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "            imgs.append(img.cpu().numpy())\n",
    "        return imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=16, channels=3):\n",
    "        return self.p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqPRcjixzMYF"
   },
   "source": [
    "## 4. Our simple dataset: 1 image of a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03Ej_9U5yBV_"
   },
   "outputs": [],
   "source": [
    "transform = tfs.Compose( # PIL -> torch\n",
    "    [\n",
    "        tfs.Resize((64, 64)),\n",
    "        tfs.ToTensor(),\n",
    "        # by default our image is scaled from 0 to 1, but we need it to be from -1 to 1\n",
    "        # use a Lambda transform to scale the image\n",
    "        tfs.Lambda(lambda t: (t*2) - 1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "reverse_transform = tfs.Compose( # torch -> PIL\n",
    "    [\n",
    "        tfs.Lambda(lambda t: (t+1)/2),\n",
    "        # torch tensors are (C x H x W) but PIL expects (H x W x C)\n",
    "        tfs.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "        tfs.Lambda(lambda t: t*255.),\n",
    "        tfs.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        tfs.ToPILImage()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def get_cat_image():\n",
    "    url = 'https://upload.wikimedia.org/wikipedia/commons/6/64/Ragdoll_from_Gatil_Ragbelas.jpg'\n",
    "    filename = 'cat.jpg'\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    raw_image = PIL.Image.open(filename)\n",
    "    return raw_image\n",
    "\n",
    "def get_noisy_image(diffusion, x_start, t):\n",
    "    # add noise\n",
    "    x_noisy = diffusion.q_sample(x_start, t=t)\n",
    "\n",
    "    # turn back into PIL image\n",
    "    noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "cat_torch_img = transform(get_cat_image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Abvbv_hByEIh"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use seed for reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "def plot(imgs, image_orig=None, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    _, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [image_orig] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "_HawprcI2nQu",
    "outputId": "a5962c13-19a0-4797-cedd-109e23bbee9d"
   },
   "outputs": [],
   "source": [
    "plot([get_noisy_image(Diffusion(), cat_torch_img, torch.tensor([t])) for t in [0, 9, 19, 49, 99, 199, 299]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjZcvbTFzMYG"
   },
   "source": [
    "## 5. Training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cwz8DRIl3xhT",
    "outputId": "04ef7f85-38d1-4eb5-a5bb-e0fcd99349f5"
   },
   "outputs": [],
   "source": [
    "device = torch.device('mps' if  torch.backends.mps.is_available() else 'cpu')\n",
    "print('Using device: ', device)\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "num_channels = 3\n",
    "batch_per_epoch = 10\n",
    "batch_size = 16\n",
    "image_size = 64\n",
    "epochs = 100\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader([cat_torch_img] * batch_per_epoch * batch_size,\n",
    "                                         batch_size=batch_size,\n",
    "                                         num_workers=1,\n",
    "                                         drop_last=True,\n",
    "                                         shuffle=True)\n",
    "\n",
    "diffusion = Diffusion()\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=num_channels,\n",
    "    dim_mults=(1, 2, 4)\n",
    ")\n",
    "# model = UNet(img_channels=num_channels)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def p_losses(diffusion, denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = diffusion.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t)\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for step, (batch) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = batch.shape[0]\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Algorithm 1 line 3: sample t uniformally for every example in the batch\n",
    "        t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "        loss = p_losses(diffusion, model, batch, t, loss_type=\"huber\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0 or epoch == epochs - 1 and epoch != 0:\n",
    "        print(\"Epoch:\", epoch, \"Loss:\", loss.item())\n",
    "        # Show an image.\n",
    "        with torch.no_grad():\n",
    "            samples = diffusion.sample(model, image_size=image_size, batch_size=1, channels=num_channels)\n",
    "            fig = plt.figure(figsize=(10, 2))\n",
    "            for subplot_idx, time_idx in zip([0, 1, 2, 3, 4, 5, 6], [0, 99, 199, 249, 279, 289, 299]):\n",
    "                ax = fig.add_subplot(1, 7, subplot_idx + 1)\n",
    "                sampled_img = np.transpose(samples[time_idx][0], (1, 2, 0))\n",
    "                ax.imshow(np.clip((sampled_img + 1)/2, 0, 1))\n",
    "                ax.set_axis_off()\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0dWmVZiZNCY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cdvae]",
   "language": "python",
   "name": "conda-env-.conda-cdvae-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
